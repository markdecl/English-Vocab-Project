{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\MdeCL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MdeCL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MdeCL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import time\n",
    "import string\n",
    "\n",
    "import requests\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "eng_stop_words = set(stopwords.words('english'))\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "from pyinflect import getAllInflections, getInflection\n",
    "#from pymystem3 import Mystem\n",
    "#m = Mystem()\n",
    "from nltk.stem.snowball import SnowballStemmer \n",
    "ru_stemmer = SnowballStemmer(\"russian\") \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "english_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import urllib.request\n",
    "from wiktionaryparser import WiktionaryParser\n",
    "import json\n",
    "#from google.cloud import translate\n",
    "# GOOGLE_APPLICATION_CREDENTIALS=\"/Users/markdecourcyling/Desktop/Translating_collocations.json\"\n",
    "#translate_client = translate.Client.from_service_account_json(support_files_dir + '/Translating_collocations.json')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from translate.storage.tmx import tmxfile\n",
    "import seaborn as sns\n",
    "\n",
    "import genanki"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCTIONS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def monoling_wikt_scrape(word):\n",
    "    url = \"https://en.wiktionary.org/w/index.php?title=\" + word + '&printable=yes'\n",
    "    response = requests.get(url)\n",
    "    wikt_definitions = []\n",
    "    try:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        if soup.find('ol') != None:\n",
    "            entries = soup.find('ol')\n",
    "            #if entries.findAll('li') != None:\n",
    "            if entries.select('ol > li') != None:\n",
    "                #number_of_defs = len(entries.findAll('li'))\n",
    "                number_of_defs = len(entries.select('ol > li'))\n",
    "                index = 0\n",
    "                while index + 1 <= number_of_defs:\n",
    "    \n",
    "                    # definition:\n",
    "                    entry = entries.select('ol > li')[index]\n",
    "                    entry_text = entry.get_text()\n",
    "\n",
    "                    if entry_text != '':\n",
    "\n",
    "                        # register tag:           \n",
    "                            \n",
    "                        if entry.find('ul') != None:\n",
    "                            sent_info_tag = entry.find('ul')\n",
    "                            sent_info_text = sent_info_tag.get_text()\n",
    "                            entry_text = entry_text.replace(sent_info_text, '')\n",
    "                            \n",
    "                            \n",
    "                        ex_sent_text = ''\n",
    "                        if entry.find('dd') != None:\n",
    "                            ex_sent_tag = entry.find('dl')\n",
    "                            ex_sent_text = ex_sent_tag.get_text()\n",
    "                            entry_text = entry_text.replace(ex_sent_text, '')\n",
    "\n",
    "                        capitalised_form = word.replace(word[0], word[0].upper())\n",
    "\n",
    "                        ex_sent_with_blank = ex_sent_text.replace(word, '_____')\n",
    "                        ex_sent_with_blank = ex_sent_with_blank.replace(capitalised_form, '_____')\n",
    "                        ex_sent_full = ex_sent_text.replace(word, '<b>' + word + '</b>')\n",
    "                        ex_sent_full = ex_sent_text.replace(capitalised_form, '<b>' + capitalised_form + '</b>')\n",
    "\n",
    "                        definition_package = {'definition' : entry_text,\n",
    "                                              'ex_sentence_blank' : ex_sent_with_blank,\n",
    "                                                'ex_sentence_full' : ex_sent_full\n",
    "                                             }\n",
    "\n",
    "                        wikt_definitions.append(definition_package)\n",
    "                    \n",
    "                    index += 1\n",
    "                    \n",
    "                return(wikt_definitions)\n",
    "            else:\n",
    "                return('Error')\n",
    "        else:\n",
    "            return('Error')\n",
    "    except Exception as e:\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Error'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monoling_wikt_scrape('homogolation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def synset_syns(word):\n",
    "    syn_results = wordnet.synsets(word)\n",
    "    syns = []\n",
    "    for syn in syn_results:\n",
    "        for lm in syn.lemmas():\n",
    "            syn_word = lm.name()\n",
    "            syns.append(syn_word)\n",
    "    return syns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cataclysmal', 'cataclysmic']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset_syns('cataclysmic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def skell_synonyms_scrape(word):\n",
    "    base_url = \"https://skell.sketchengine.co.uk/run.cgi/thesaurus?lpos=&query=\"\n",
    "    response = requests.get(\"{url}/{endpoint}\".format(url=base_url, endpoint=word))\n",
    "    \n",
    "    syns = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        if soup.find('div', {'id' : 'thes_content'}) != None:\n",
    "            thes_content = soup.find('div', {'id' : 'thes_content'})\n",
    "            \n",
    "            syns = []\n",
    "            \n",
    "            syn_idx = 0\n",
    "            \n",
    "            while syn_idx + 1 <= 6:\n",
    "                \n",
    "                try:\n",
    "                    if thes_content.findAll('li')[syn_idx].get_text() != None:\n",
    "                        syn = thes_content.findAll('li')[syn_idx].get_text()\n",
    "                        syns.append(syn)\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                syn_idx += 1\n",
    "        else:\n",
    "            synonyms_text = \"No synonyms found.\"\n",
    "    else:\n",
    "        print('\\tSynonym scraper couldn\\'t connect to Skell for ' + word + '.')\n",
    "        \n",
    "    return(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skell_synonyms_scrape('mimetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def skell_example_sentence_scrape(word):\n",
    "    base_url = \"https://skell.sketchengine.co.uk/run.cgi/concordance?lpos=&query=\"\n",
    "    response = requests.get(\"{url}/{endpoint}\".format(url=base_url, endpoint=word))\n",
    "    print(response)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    ex_sents_blank = ''\n",
    "    ex_sents_full = ''\n",
    "    content_div = soup.find('div', {'id' : 'content'})\n",
    "    if content_div.find('tr') != None:\n",
    "        for tr_div in content_div.find_all('tr', limit = 4):\n",
    "            for sent_div in tr_div.find_all('td'):\n",
    "                if sent_div.has_attr(\"class\") == True:\n",
    "                    pass\n",
    "                else:\n",
    "                    if sent_div.find('span', {'class':'lc'}) != None:\n",
    "                        pre_term = sent_div.find('span', {'class':'lc'})\n",
    "                        pre_term_text = pre_term.get_text()\n",
    "                    else:\n",
    "                        pre_term_text = ''\n",
    "                        \n",
    "                    if sent_div.find('span', {'class':'kw'}) != None:\n",
    "                        term = sent_div.find('span', {'class':'kw'})\n",
    "                        term_text = term.get_text().strip()\n",
    "                        \n",
    "                    if sent_div.find('span', {'class':'rc'}) != None:\n",
    "                        post_term = sent_div.find('span', {'class':'rc'})\n",
    "                        post_term_text = post_term.get_text()\n",
    "                    else:\n",
    "                        post_term_text = ''\n",
    "                    ex_sent_blank = pre_term_text + ' _______ ' + post_term_text\n",
    "                    ex_sent_full = pre_term_text + ' <b>' + term_text + '</b>' + post_term_text\n",
    "\n",
    "                    ex_sents_blank += ex_sent_blank + '<br><br>'\n",
    "                    ex_sents_full += ex_sent_full + '<br><br>'\n",
    "            \n",
    "    else:\n",
    "        example_sentence_text = \"No example sentence found.\"\n",
    "\n",
    "    return{'Example sentences with blanks' : ex_sents_blank,\n",
    "           'Example sentences full' : ex_sents_full\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-344d7a78e2f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mskell_example_sentence_scrape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-290d148ace6d>\u001b[0m in \u001b[0;36mskell_example_sentence_scrape\u001b[1;34m(word)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mex_sents_full\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mcontent_div\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;34m'content'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mcontent_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtr_div\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontent_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlimit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0msent_div\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtr_div\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'td'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "skell_example_sentence_scrape('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def skell_ex_sents_scrape(word):\n",
    "    \n",
    "    base_url = \"https://skell.sketchengine.co.uk/run.cgi/thesaurus?lpos=&query=\"\n",
    "    response = requests.get(\"{url}/{endpoint}\".format(url=base_url, endpoint=word))\n",
    "    \n",
    "    ex_sents_blank = []\n",
    "    ex_sents_full = []\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    return {'Example sentences with blanks' : ex_sents_blank,\n",
    "           'Example sentences full' : ex_sents_full}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Example sentences with blanks': [], 'Example sentences full': []}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skell_ex_sents_scrape('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the flashcards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_type = genanki.Model(\n",
    "  1603245899,\n",
    "  'Monolingual English Vocab Test Note Type',\n",
    "  fields = [\n",
    "    {'name': 'Term'},\n",
    "    {'name': 'First letter'},\n",
    "    {'name' : 'Wikt defs blanks'},\n",
    "    {'name' : 'Wikt defs full'},\n",
    "    {'name' : 'Syns'},\n",
    "    {'name': 'Ex sents with blanks'},\n",
    "    {'name': 'Ex sents full'}\n",
    "  ],\n",
    "  templates = [\n",
    "    {\n",
    "      'name': 'Monolingual English Vocab Test Card',\n",
    "      'qfmt': '<font color=\"navy\" size=\"+2\">{{Syns}}</font><br><hr><font size=\"+1\">{{Ex sents with blanks}}<br>{{type:Term}}</font><hr><i><font color=\"firebrick\">{{Wikt defs blanks}}<br><br><br><br><br><br><br><br><br><br><font size=\"-3\">First letter:<br>{{First letter}}</font></i>',\n",
    "      'afmt': '<font color=\"navy\" size=\"+2\">{{Syns}}</font><br><hr><font size=\"+1\">{{Ex sents full}}<br>{{type:Term}}</font><hr><i><font color=\"firebrick\">{{Wikt defs blanks}}<br><br><br><br><br><br><br><br><br><br><font size=\"-3\">First letter:<br>{{First letter}}</font></i>'\n",
    "    }\n",
    "  ]\n",
    "#     css='.card{font-family: arial;font-size: 20px;text-align: left;color: black;background-color: #88FFDD;}',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_deck = genanki.Deck(\n",
    "  2059400110,\n",
    "  'English Vocab Deck')\n",
    "\n",
    "word_list = []\n",
    "with open('C://Users//MdeCL//OneDrive//English Vocab Project//business_vocab.txt', 'r') as input_file:\n",
    "    word_list = input_file.readlines()\n",
    "    word_list = [item.replace('\\n', '').lower() for item in word_list]\n",
    "\n",
    "word_list = list(set(word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "feasibility study\n",
      "string indices must be integers\n",
      "1\n",
      "redouble\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "remuneration\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "homogolation\n",
      "string indices must be integers\n",
      "1\n",
      "run aground\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "horizontal integration\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "continuity plans\n",
      "string indices must be integers\n",
      "1\n",
      "c-suite\n",
      "string indices must be integers\n",
      "1\n",
      "market penetration\n",
      "string indices must be integers\n",
      "1\n",
      "e-commerce\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "distribution partner\n",
      "string indices must be integers\n",
      "1\n",
      "data lake\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "ideate\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "poised\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "business model\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "upend\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "b2b\n",
      "string indices must be integers\n",
      "1\n",
      "professional services\n",
      "string indices must be integers\n",
      "1\n",
      "mass market adoption\n",
      "string indices must be integers\n",
      "1\n",
      "double down\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "acquisition\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "financial modelling \n",
      "string indices must be integers\n",
      "1\n",
      "data hub\n",
      "string indices must be integers\n",
      "1\n",
      "sharing economy\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "vertical integration\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "due diligence\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "nexus\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "bootstrapped\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "1\n",
      "b2c\n",
      "string indices must be integers\n",
      "1\n",
      "insolvent\n",
      "<Response [200]>\n",
      "'NoneType' object has no attribute 'find'\n",
      "\n",
      "\n",
      "Flashcard writing complete.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "for term in word_list:\n",
    "    if term != '':\n",
    "        print(idx)\n",
    "        print(term)\n",
    "        try:\n",
    "            \n",
    "            first_letter = term[0]\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            wikt_defs = monoling_wikt_scrape(term)\n",
    "            wikt_defs_blanks = ''\n",
    "            wikt_defs_full = ''\n",
    "            for wikt_def in wikt_defs:\n",
    "                wikt_defs_blanks += '<font size=\"+2\">' + wikt_def['definition'] + '<br></font>'\n",
    "                wikt_defs_full += '<font size=\"+2\">' + wikt_def['definition'] + '<br></font>'\n",
    "                wikt_defs_blanks += wikt_def['ex_sentence_blank'] + '<br><br>'\n",
    "                wikt_defs_full += wikt_def['ex_sentence_full'] + '<br><br>'\n",
    "\n",
    "            syns = ', '.join(skell_synonyms_scrape(term))\n",
    "\n",
    "            ex_sents = skell_example_sentence_scrape(term)\n",
    "            ex_sents_blanks = ex_sents['Example sentences with blanks']\n",
    "            ex_sents_full = ex_sents['Example sentences full']\n",
    "\n",
    "            note = genanki.Note(\n",
    "                            model = note_type,\n",
    "                            fields = [term,\n",
    "                                      first_letter,\n",
    "                                      wikt_defs_blanks,\n",
    "                                      wikt_defs_full,\n",
    "                                      syns,\n",
    "                                      ex_sents_blanks,\n",
    "                                      ex_sents_full\n",
    "                                        ]\n",
    "                            )\n",
    "\n",
    "            my_deck.add_note(note)\n",
    "            idx += 1\n",
    "            \n",
    "            end_time = time.time()\n",
    "            time_taken = end_time - start_time\n",
    "            if time_taken < 3:\n",
    "                time.sleep(3 - time_taken)\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "genanki.Package(my_deck).write_to_file('C:\\\\Users\\\\MdeCL\\\\Desktop\\\\monoling_eng_vocab.apkg')\n",
    "print('\\n\\nFlashcard writing complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "genanki.Package(my_deck).write_to_file('C:\\\\Users\\\\MdeCL\\\\Desktop\\\\monoling_eng_vocab.apkg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/markdecourcyling/Desktop/english_lit_vocab_flashcards.txt', 'w+') as file:\n",
    "    with open('/Users/markdecourcyling/Desktop/english_lit_vocab.txt', 'r') as word_list:\n",
    "        word_list_read = word_list.read()\n",
    "        word_list = word_list_read.split('\\n')\n",
    "    \n",
    "    word_list = list(set(word_list))\n",
    "    language = 'english'\n",
    "    parser = WiktionaryParser()\n",
    "    for word in word_list:\n",
    "        word = word.lower()\n",
    "        if parser.fetch(word, language) != None:\n",
    "            if parser.fetch(word, language):\n",
    "                try:\n",
    "                    data = parser.fetch(word, language)[0]\n",
    "                    #time.sleep(2)\n",
    "                    definitions = data['definitions'][0]['text']\n",
    "                    item_idx = 0\n",
    "                    def_list = definitions[1:]\n",
    "                    for definition in def_list:\n",
    "                        definitions_with_blank = ''\n",
    "                        definitions_full = ''\n",
    "                        for item in def_list:\n",
    "                            if definition == item:\n",
    "                                definitions_with_blank += '<b>' + '???????' + '</b><br><br>'\n",
    "                                definitions_full += '<b>' + item + '</b><br><br>'\n",
    "                            else:\n",
    "                                definitions_with_blank += item + '<br><br>'\n",
    "                                definitions_full += item + '<br><br>'\n",
    "                        print(word + '£<br>' + definitions_with_blank + '£<br>' + definitions_full + '\\n')\n",
    "                        file.write(word + '£<br>' + definitions_with_blank + '£<br>' + definitions_full + '\\n')\n",
    "                        test_recall_flashcards.write(word + '£<br>' + definitions_full + '\\n')\n",
    "                        item_idx += 1\n",
    "                except IndexError:\n",
    "                    print('COULDN\\'T WRITE CARDS FOR:')\n",
    "                    print(word)\n",
    "                    pass\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/markdecourcyling/Desktop/english_lit_vocab_flashcards.txt', 'w+') as file:\n",
    "    \n",
    "    with open('/Users/markdecourcyling/Desktop/english_lit_vocab.txt', 'r') as word_list:\n",
    "        word_list_read = word_list.read()\n",
    "        word_list = word_list_read.split('\\n')\n",
    "    \n",
    "    for word in word_list:\n",
    "        dict_defs = monoling_wikt_scrape(word)\n",
    "        \n",
    "        print(dict_defs)\n",
    "        \n",
    "        if dict_defs != 'Error':\n",
    "\n",
    "            item_idx = 0\n",
    "            for dict_definition in dict_defs:\n",
    "                \n",
    "                monoling_defs_blank = ''\n",
    "                monoling_defs_complete = ''\n",
    "                \n",
    "                for item in dict_defs:\n",
    "\n",
    "                    definition = dict_definition['definition']\n",
    "                    ex_sent_blank = dict_definition['ex_sentence_blank']\n",
    "                    ex_sent_full = dict_definition['ex_sentence_full']\n",
    "\n",
    "                    if dict_defs.index(item) == item_idx:\n",
    "                        monoling_defs_blank += '<b>' + '???????' + '<br><emsp>' + ex_sent_blank + '</b><br><br>'\n",
    "\n",
    "                        monoling_defs_complete += '<b>' + definition + '<br><emsp>' + ex_sent_blank + '</b><br><br>'\n",
    "\n",
    "                    else:\n",
    "                        monoling_defs_blank += definition + '<br><emsp>' + ex_sent_blank + '<br><br>'\n",
    "\n",
    "                        monoling_defs_complete += definition + '<br><emsp>' + ex_sent_full + '<br><br>'\n",
    "\n",
    "\n",
    "                    print(word + '£<br>' + monoling_defs_blank + '£<br>' + monoling_defs_complete + '\\n')\n",
    "\n",
    "                    file.write(word + '£<br>' + monoling_defs_blank + '£<br>' + monoling_defs_complete + '\\n')\n",
    "\n",
    "                    item_idx += 1\n",
    "    \n",
    "monoling_wikt_scrape('admonition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def wiktionary_parser_eng_trans_scrape(word):\n",
    "    language = 'english'\n",
    "\n",
    "    parser = WiktionaryParser()\n",
    "    if parser.fetch(word, language) != None:\n",
    "        if parser.fetch(word, language) != None:\n",
    "            data = parser.fetch(word, language)\n",
    "            try:\n",
    "                definitions = data[0]['definitions']  \n",
    "                try:\n",
    "                    definitions_list = definitions[0]['text']\n",
    "                    definitions_list = definitions_list[1:]\n",
    "                    for definition in definitions_list:\n",
    "                        if 'Alternative spelling of ' in definition:\n",
    "                            alt_spelling = definition.split('Alternative spelling of ')[1]\n",
    "                            # print(alt_spelling.split())\n",
    "                            # print(alt_spelling.split()[0])\n",
    "                            alt_spelling = alt_spelling.split()[0]\n",
    "                            # print(alt_spelling)\n",
    "                            if parser.fetch(alt_spelling, language) != None:\n",
    "                                    data = parser.fetch(alt_spelling, language)\n",
    "                                    definitions = data[0]['definitions']\n",
    "                                    print('Wiktionary scraper found the alt spelling \\'' + alt_spelling + '\\' and returned its definitions.')\n",
    "                                    if definitions:\n",
    "                                        definitions_list = definitions[0]['text']\n",
    "                                        return_list = definitions_list[1:]\n",
    "                                    else:\n",
    "                                        return_list = ['WiktionaryParser can\\'t find definitions for \\'' + alt_spelling + '\\'.']\n",
    "                        else:\n",
    "                            return_list = definitions_list\n",
    "                            \n",
    "                except Exception:\n",
    "                    return_list = ['No', 'translations', 'found']\n",
    "            except Exception:\n",
    "                return_list = ['No', 'translations', 'found']\n",
    "        else:\n",
    "            # complete_definitions_text = 'WiktionaryParser can\\'t find definitions for \\'' + word + '\\'.'\n",
    "            return_list = ['No', 'translations', 'found']\n",
    "    else:\n",
    "        # complete_definitions_text = 'WiktionaryParser can\\'t find definitions for \\'' + word + '\\'.'\n",
    "        return_list = ['No', 'translations', 'found']\n",
    "    \n",
    "    return(return_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit the flashards to make more guessable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\MdeCL\\\\OneDrive\\\\English Vocab Project\\\\1 - English Writing Skills__English Vocab (Early Easter Vac 2020).txt', 'r', encoding=\"utf8\") as file:\n",
    "    with open('C:\\\\Users\\\\MdeCL\\\\OneDrive\\\\English Vocab Project\\\\1 - English Writing Skills__English Vocab (Early Easter Vac 2020) 2.txt', 'w', encoding=\"utf8\") as output_file:\n",
    "        \n",
    "        for line in file.readlines():\n",
    "\n",
    "            line_list = line.split('\\t')\n",
    "            \n",
    "            first_letter = line_list[0][0]\n",
    "            # PUT THE FIRST LETTER IN FRONT OF EACH BLANK\n",
    "#             line_list = [re.sub(r'_+', first_letter + '______', item) for item in line_list]\n",
    "            \n",
    "            # PUT THE FIRST LETTER AT THE BOTTOM OF THE CARD, SO I HAVE TO SCROLL TO SEE IT.\n",
    "            line_list[1] = line_list[1] + '<br><br><br><br><br><br><br><br><br><br><br>' + 'First letter:' + '<br>' + first_letter\n",
    "\n",
    "            output_file.write('\\t'.join(line_list) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
